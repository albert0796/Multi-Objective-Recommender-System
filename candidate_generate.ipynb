{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":38760,"databundleVersionId":4493939,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10926253,"sourceType":"datasetVersion","datasetId":6793043},{"sourceId":10926321,"sourceType":"datasetVersion","datasetId":6793106},{"sourceId":10926720,"sourceType":"datasetVersion","datasetId":6793437},{"sourceId":10926773,"sourceType":"datasetVersion","datasetId":6793482},{"sourceId":10945065,"sourceType":"datasetVersion","datasetId":6807528},{"sourceId":10974557,"sourceType":"datasetVersion","datasetId":6828978},{"sourceId":10974819,"sourceType":"datasetVersion","datasetId":6829138},{"sourceId":4436180,"sourceType":"datasetVersion","datasetId":2597726}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# co-visitation","metadata":{}},{"cell_type":"code","source":"VER = 5\n\nimport pandas as pd, numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport cudf, itertools\nprint('We will use RAPIDS version',cudf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:32:53.084355Z","iopub.execute_input":"2025-03-13T00:32:53.084631Z","iopub.status.idle":"2025-03-13T00:32:59.414450Z","shell.execute_reply.started":"2025-03-13T00:32:53.084595Z","shell.execute_reply":"2025-03-13T00:32:59.413611Z"}},"outputs":[{"name":"stdout","text":"We will use RAPIDS version 25.02.00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%time\ndef read_file(f):\n    return cudf.DataFrame( data_cache[f] )\ndef read_file_to_cache(f):\n    df = pd.read_parquet(f)\n    df.ts = (df.ts/1000).astype('int32')\n    df['type'] = df['type'].map(type_labels).astype('int8')\n    return df\n    \ndata_cache = {}\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\nfiles = glob.glob('/kaggle/input/otto-chunk-data-inparquet-format/test_parquet/*')+glob.glob('/kaggle/input/otto-chunk-data-inparquet-format/train_parquet/*')\nfor f in files: data_cache[f] = read_file_to_cache(f)\n\nREAD_CT = 5\nCHUNK = int( np.ceil( len(files)/6 ))\nprint(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:32:59.415196Z","iopub.execute_input":"2025-03-13T00:32:59.415586Z","iopub.status.idle":"2025-03-13T00:33:47.225449Z","shell.execute_reply.started":"2025-03-13T00:32:59.415562Z","shell.execute_reply":"2025-03-13T00:33:47.224707Z"}},"outputs":[{"name":"stdout","text":"We will process 146 files, in groups of 5 and chunks of 25.\nCPU times: user 43.6 s, sys: 9.16 s, total: 52.8 s\nWall time: 47.8 s\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%time\ntype_weight = {0:1, 1:6, 2:3}\n\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n    tmp.to_pandas().to_parquet(f'top_40_carts_orders_v{VER}_{PART}.pqt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:33:47.226215Z","iopub.execute_input":"2025-03-13T00:33:47.226537Z","iopub.status.idle":"2025-03-13T00:37:54.157201Z","shell.execute_reply.started":"2025-03-13T00:33:47.226499Z","shell.execute_reply":"2025-03-13T00:37:54.156098Z"}},"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 3min 36s, sys: 29.3 s, total: 4min 6s\nWall time: 4min 6s\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%time\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.loc[df['type'].isin([1,2])]\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)\n    tmp.to_pandas().to_parquet(f'top_40_buy2buy_v{VER}_{PART}.pqt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:37:54.157958Z","iopub.execute_input":"2025-03-13T00:37:54.158168Z","iopub.status.idle":"2025-03-13T00:38:52.673734Z","shell.execute_reply.started":"2025-03-13T00:37:54.158150Z","shell.execute_reply":"2025-03-13T00:38:52.672884Z"}},"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 47.3 s, sys: 11.1 s, total: 58.4 s\nWall time: 58.5 s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%time\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        for k in range(a,b,READ_CT):\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<45].drop('n',axis=1)\n    tmp.to_pandas().to_parquet(f'top_45_clicks_v{VER}_{PART}.pqt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:38:52.675489Z","iopub.execute_input":"2025-03-13T00:38:52.675751Z","iopub.status.idle":"2025-03-13T00:42:59.716086Z","shell.execute_reply.started":"2025-03-13T00:38:52.675731Z","shell.execute_reply":"2025-03-13T00:42:59.715182Z"}},"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 3min 38s, sys: 27.5 s, total: 4min 6s\nWall time: 4min 7s\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# FREE MEMORY\ndel data_cache, tmp\n_ = gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:42:59.717130Z","iopub.execute_input":"2025-03-13T00:42:59.717374Z","iopub.status.idle":"2025-03-13T00:43:00.135983Z","shell.execute_reply.started":"2025-03-13T00:42:59.717354Z","shell.execute_reply":"2025-03-13T00:43:00.135138Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# generate candidates","metadata":{}},{"cell_type":"code","source":"type_labels = {'clicks':0, 'carts':1, 'orders':2}\ndef load_test():    \n    dfs = []\n    for e, chunk_file in enumerate(glob.glob('/kaggle/input/otto-chunk-data-inparquet-format/test_parquet/*')):\n        chunk = pd.read_parquet(chunk_file)\n        chunk.ts = (chunk.ts/1000).astype('int32')\n        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n        dfs.append(chunk)\n    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n\ntest_df = load_test()\nprint('Test data has shape',test_df.shape)\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:46:46.068819Z","iopub.execute_input":"2025-03-13T00:46:46.069118Z","iopub.status.idle":"2025-03-13T00:46:47.074043Z","shell.execute_reply.started":"2025-03-13T00:46:46.069098Z","shell.execute_reply":"2025-03-13T00:46:47.073353Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\nTest data has shape (6928123, 4)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"    session     aid          ts  type\n0  13099779  245308  1661795832     0\n1  13099779  245308  1661795862     1\n2  13099779  972319  1661795888     0\n3  13099779  972319  1661795898     1\n4  13099779  245308  1661795907     0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session</th>\n      <th>aid</th>\n      <th>ts</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13099779</td>\n      <td>245308</td>\n      <td>1661795832</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13099779</td>\n      <td>245308</td>\n      <td>1661795862</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13099779</td>\n      <td>972319</td>\n      <td>1661795888</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13099779</td>\n      <td>972319</td>\n      <td>1661795898</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13099779</td>\n      <td>245308</td>\n      <td>1661795907</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def load_full_train():    \n    dfs = []\n    files_a = glob.glob('/kaggle/input/otto-chunk-data-inparquet-format/test_parquet/*')\n    files_b = glob.glob('/kaggle/input/otto-chunk-data-inparquet-format/train_parquet/*')\n    all_files = files_a + files_b\n    for e, chunk_file in enumerate(all_files):\n        chunk = pd.read_parquet(chunk_file)\n        chunk.ts = (chunk.ts/1000).astype('int32')\n        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n        dfs.append(chunk)\n    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n\nfull_train_df = load_full_train()\nprint('full_train data has shape',full_train_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:46:53.522401Z","iopub.execute_input":"2025-03-13T00:46:53.522700Z","iopub.status.idle":"2025-03-13T00:47:27.540888Z","shell.execute_reply.started":"2025-03-13T00:46:53.522678Z","shell.execute_reply":"2025-03-13T00:47:27.540122Z"}},"outputs":[{"name":"stdout","text":"full_train data has shape (223644219, 4)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"top_clicks = test_df.loc[test_df['type']==0,'aid'].value_counts().index.values[:20]\ntop_orders = test_df.loc[test_df['type']==2,'aid'].value_counts().index.values[:20]\nfull_top_orders = full_train_df.loc[full_train_df['type']==2,'aid'].value_counts().index.values[:200]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:47:27.541974Z","iopub.execute_input":"2025-03-13T00:47:27.542288Z","iopub.status.idle":"2025-03-13T00:47:28.453252Z","shell.execute_reply.started":"2025-03-13T00:47:27.542257Z","shell.execute_reply":"2025-03-13T00:47:28.452604Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"%%time\nDISK_PIECES = 4\ndef pqt_to_dict(df):\n    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n# LOAD THREE CO-VISITATION MATRICES\ntop_20_clicks = pqt_to_dict( pd.read_parquet(f'top_45_clicks_v5_0.pqt') )\nfor k in range(1,DISK_PIECES): \n    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_45_clicks_v{VER}_{k}.pqt') ) )\n    \ntop_20_buys = pqt_to_dict( pd.read_parquet(f'top_40_carts_orders_v{VER}_0.pqt') )\nfor k in range(1,DISK_PIECES): \n    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_40_carts_orders_v{VER}_{k}.pqt') ) )\n    \ntop_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_40_buy2buy_v{VER}_0.pqt') )\nfor k in range(1,DISK_PIECES): \n    top_20_buy2buy.update( pqt_to_dict( pd.read_parquet(f'top_40_buy2buy_v{VER}_{k}.pqt') ) )\n\n\n\nprint('Here are size of our 3 co-visitation matrices:')\nprint( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T01:00:44.050056Z","iopub.execute_input":"2025-03-13T01:00:44.050361Z","iopub.status.idle":"2025-03-13T01:02:39.183119Z","shell.execute_reply.started":"2025-03-13T01:00:44.050336Z","shell.execute_reply":"2025-03-13T01:02:39.182238Z"}},"outputs":[{"name":"stdout","text":"Here are size of our 3 co-visitation matrices:\n1837166 1168768 1837166\nCPU times: user 1min 57s, sys: 10.3 s, total: 2min 7s\nWall time: 1min 55s\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n\ndef suggest_clicks(df):\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    if len(unique_aids)>=50:\n        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        sorted_aids = [k for k,v in aids_temp.most_common(50)]\n        return sorted_aids\n    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(50) if aid2 not in unique_aids]    \n    result = unique_aids + top_aids2[:50 - len(unique_aids)]\n    return result + list(top_clicks)[:50-len(result)]\n\ndef suggest_buys(df):\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    df = df.loc[(df['type']==1)|(df['type']==2)]\n    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n    if len(unique_aids)>=50:\n        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n        for aid in aids3: aids_temp[aid] += 0.1\n        sorted_aids = [k for k,v in aids_temp.most_common(50)]\n        return sorted_aids\n    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(50) if aid2 not in unique_aids] \n    result = unique_aids + top_aids2[:50 - len(unique_aids)]\n    return result + list(top_orders)[:50-len(result)]\n\ndef suggest_candidates_50(df):\n    cand_clicks = suggest_clicks(df)\n    cand_buys   = suggest_buys(df)\n\n    merged = cand_clicks + cand_buys\n    merged_unique = list(dict.fromkeys(merged))\n\n    if len(merged_unique) < 50:\n        needed = 50 - len(merged_unique)\n        fill_items = []\n        for item in full_top_orders:\n            if item not in merged_unique:\n                fill_items.append(item)\n                if len(fill_items) >= needed:\n                    break\n        merged_unique += fill_items\n\n    return merged_unique[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T01:05:57.216822Z","iopub.execute_input":"2025-03-13T01:05:57.217161Z","iopub.status.idle":"2025-03-13T01:05:57.227945Z","shell.execute_reply.started":"2025-03-13T01:05:57.217136Z","shell.execute_reply":"2025-03-13T01:05:57.227163Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"candidate_rows = []\nfor session_id, subdf in test_df.groupby('session', sort=False):\n    cands_50 = suggest_candidates_50(subdf)\n    for aid in cands_50:\n        candidate_rows.append((session_id, aid))\n\ncandidate_df = pd.DataFrame(candidate_rows, columns=['session','aid'])\nprint(candidate_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T01:06:05.428881Z","iopub.execute_input":"2025-03-13T01:06:05.429170Z","iopub.status.idle":"2025-03-13T01:27:49.970898Z","shell.execute_reply.started":"2025-03-13T01:06:05.429150Z","shell.execute_reply":"2025-03-13T01:27:49.969613Z"}},"outputs":[{"name":"stdout","text":"(83590150, 2)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\nprint(candidate_df.head(200))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T01:27:49.971972Z","iopub.execute_input":"2025-03-13T01:27:49.972240Z","iopub.status.idle":"2025-03-13T01:27:49.980790Z","shell.execute_reply.started":"2025-03-13T01:27:49.972220Z","shell.execute_reply":"2025-03-13T01:27:49.979927Z"}},"outputs":[{"name":"stdout","text":"      session      aid\n0    13099779   245308\n1    13099779   972319\n2    13099779  1100749\n3    13099779  1090801\n4    13099779   717801\n5    13099779   885958\n6    13099779  1734314\n7    13099779   352868\n8    13099779   128649\n9    13099779   595204\n10   13099779   319580\n11   13099779   116651\n12   13099779  1552337\n13   13099779  1575802\n14   13099779   441353\n15   13099779   745721\n16   13099779   428539\n17   13099779  1332889\n18   13099779  1801838\n19   13099779  1303193\n20   13099779   587671\n21   13099779   908814\n22   13099779  1491863\n23   13099779   798152\n24   13099779  1627951\n25   13099779  1732664\n26   13099779  1132027\n27   13099779  1534310\n28   13099779   942224\n29   13099779  1311574\n30   13099779  1270528\n31   13099779  1633488\n32   13099779   803272\n33   13099779   970197\n34   13099779   240614\n35   13099779  1481491\n36   13099779   224593\n37   13099779   727122\n38   13099779  1482503\n39   13099779  1082306\n40   13099779  1710382\n41   13099779   843086\n42   13099779  1144446\n43   13099779  1774346\n44   13099779   188086\n45   13099779   728425\n46   13099779   759476\n47   13099779  1617701\n48   13099779   165160\n49   13099779   663175\n50   13099780   354638\n51   13099780  1308930\n52   13099780   604085\n53   13099780   108125\n54   13099780   612920\n55   13099780   970075\n56   13099780  1452417\n57   13099780  1187917\n58   13099780   912260\n59   13099780  1557927\n60   13099780  1192896\n61   13099780   363506\n62   13099780  1658764\n63   13099780   100552\n64   13099780   840603\n65   13099780   291229\n66   13099780  1673641\n67   13099780   738098\n68   13099780  1139287\n69   13099780   846336\n70   13099780  1812932\n71   13099780  1411340\n72   13099780     7651\n73   13099780  1314325\n74   13099780     1089\n75   13099780  1725311\n76   13099780    90309\n77   13099780   659399\n78   13099780   435253\n79   13099780  1713017\n80   13099780    39615\n81   13099780    24318\n82   13099780   522226\n83   13099780   272435\n84   13099780   988906\n85   13099780  1402537\n86   13099780   543961\n87   13099780  1446905\n88   13099780   847816\n89   13099780   101491\n90   13099780   164404\n91   13099780  1440439\n92   13099780  1819003\n93   13099780   435445\n94   13099780  1512713\n95   13099780  1004601\n96   13099780  1460571\n97   13099780   485256\n98   13099780   986164\n99   13099780   809578\n100  13099781  1055072\n101  13099781   669617\n102  13099781   456362\n103  13099781  1430828\n104  13099781    13552\n105  13099781   423142\n106  13099781  1419354\n107  13099781   205910\n108  13099781   947874\n109  13099781  1412451\n110  13099781   608514\n111  13099781  1111219\n112  13099781   825120\n113  13099781   873293\n114  13099781   805774\n115  13099781  1428571\n116  13099781   992009\n117  13099781  1435618\n118  13099781   145234\n119  13099781  1162087\n120  13099781  1464919\n121  13099781   403522\n122  13099781   364658\n123  13099781   315426\n124  13099781   606094\n125  13099781  1582322\n126  13099781   344862\n127  13099781  1332173\n128  13099781   231476\n129  13099781    86499\n130  13099781   654809\n131  13099781   521173\n132  13099781  1631223\n133  13099781   956028\n134  13099781  1686864\n135  13099781  1622185\n136  13099781   396307\n137  13099781   625237\n138  13099781   660976\n139  13099781  1781537\n140  13099781  1736295\n141  13099781   493430\n142  13099781  1452043\n143  13099781   445110\n144  13099781   498571\n145  13099781   293359\n146  13099781   688810\n147  13099781  1559154\n148  13099781  1318431\n149  13099781    25597\n150  13099782   346676\n151  13099782  1369875\n152  13099782  1684656\n153  13099782  1434963\n154  13099782   728766\n155  13099782   426751\n156  13099782   825049\n157  13099782  1343424\n158  13099782  1620792\n159  13099782  1224329\n160  13099782  1699367\n161  13099782  1229372\n162  13099782  1581194\n163  13099782  1365991\n164  13099782  1099106\n165  13099782  1406183\n166  13099782   107206\n167  13099782  1053324\n168  13099782  1675478\n169  13099782  1830582\n170  13099782   207053\n171  13099782    89696\n172  13099782  1168074\n173  13099782  1162200\n174  13099782   615890\n175  13099782  1326116\n176  13099782   333433\n177  13099782   402356\n178  13099782  1582313\n179  13099782   677589\n180  13099782   104052\n181  13099782  1536459\n182  13099782   101187\n183  13099782  1363631\n184  13099782   843034\n185  13099782  1172748\n186  13099782   396879\n187  13099782   294447\n188  13099782  1530545\n189  13099782   869346\n190  13099782   279078\n191  13099782  1823184\n192  13099782  1290173\n193  13099782   811236\n194  13099782  1545385\n195  13099782   925165\n196  13099782   932714\n197  13099782   209809\n198  13099782   542639\n199  13099782   673407\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"candidate_df.to_parquet(\"candidates.parquet\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T01:27:49.982104Z","iopub.execute_input":"2025-03-13T01:27:49.982318Z","iopub.status.idle":"2025-03-13T01:27:55.084110Z","shell.execute_reply.started":"2025-03-13T01:27:49.982300Z","shell.execute_reply":"2025-03-13T01:27:55.082992Z"}},"outputs":[],"execution_count":18}]}