{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_table(df_candidates, df_labels, chunk_size=500000):\n",
    "    # 直接 explode() ground_truth，變成長格式\n",
    "    df_labels_exploded = df_labels.explode(\"ground_truth\").dropna()\n",
    "    df_labels_exploded = df_labels_exploded.rename(columns={\"ground_truth\": \"aid\"})\n",
    "\n",
    "    # 設置 chunking 批量處理，減少記憶體消耗\n",
    "    num_chunks = (len(df_candidates) + chunk_size - 1) // chunk_size  # 確保 chunk 數量正確\n",
    "    df_chunks = []\n",
    "\n",
    "    print(f\"Total chunks: {num_chunks}\")\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        print(f\"Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "        # 讀取 df_candidates 的部分資料\n",
    "        df_chunk = df_candidates.iloc[i * chunk_size: (i + 1) * chunk_size].copy()\n",
    "\n",
    "        # **儲存原始索引**\n",
    "        original_index = df_chunk.index.copy()\n",
    "\n",
    "        # **合併標籤資料**\n",
    "        df_chunk = df_chunk.merge(df_labels_exploded, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "        # **使用 pivot() 讓 clicks, carts, orders 變成獨立欄位**\n",
    "        df_chunk[\"label\"] = 1\n",
    "        df_chunk = df_chunk.pivot(index=[\"session\", \"aid\"], columns=\"type\", values=\"label\").reset_index()\n",
    "\n",
    "        # **確保 `clicks, carts, orders` 欄位存在**\n",
    "        # for col in [\"clicks\", \"carts\", \"orders\"]:\n",
    "        #     if col not in df_chunk.columns:\n",
    "        #         df_chunk[col] = 0\n",
    "\n",
    "        # **使用 `reindex()` 確保 `clicks, carts, orders` 欄位存在**\n",
    "        df_chunk = df_chunk.reindex(columns=[\"session\", \"aid\", \"clicks\", \"carts\", \"orders\"], fill_value=0)\n",
    "\n",
    "        # **確保 `merge()` 不影響 `df_candidates` 順序**\n",
    "        df_chunk = df_chunk.merge(df_candidates[[\"session\", \"aid\"]], on=[\"session\", \"aid\"], how=\"right\")\n",
    "        df_chunk = df_chunk.reindex(original_index)\n",
    "\n",
    "        # **填補 NaN（表示沒發生行為的標籤），並確保欄位順序**\n",
    "        df_chunk = df_chunk.fillna(0).astype(int)[[\"clicks\", \"carts\", \"orders\"]]\n",
    "\n",
    "        # **單獨存儲每個 chunk**\n",
    "        df_chunks.append(df_chunk)\n",
    "\n",
    "    # 合併所有 chunk\n",
    "    df_final = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "    df_final.to_parquet(\"data/labels.parquet\", index=False)\n",
    "    print(\"Parquet saved successfully\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/candidates_features_engineering.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/val_labels.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_candidates = pd.read_parquet(\"data/candidates_features_engineering.parquet\")\n",
    "df_labels = pd.read_parquet(\"data/val_labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 181\n",
      "Processing chunk 1/181...\n",
      "Processing chunk 2/181...\n",
      "Processing chunk 3/181...\n",
      "Processing chunk 4/181...\n",
      "Processing chunk 5/181...\n",
      "Processing chunk 6/181...\n",
      "Processing chunk 7/181...\n",
      "Processing chunk 8/181...\n",
      "Processing chunk 9/181...\n",
      "Processing chunk 10/181...\n",
      "Processing chunk 11/181...\n",
      "Processing chunk 12/181...\n",
      "Processing chunk 13/181...\n",
      "Processing chunk 14/181...\n",
      "Processing chunk 15/181...\n",
      "Processing chunk 16/181...\n",
      "Processing chunk 17/181...\n",
      "Processing chunk 18/181...\n",
      "Processing chunk 19/181...\n",
      "Processing chunk 20/181...\n",
      "Processing chunk 21/181...\n",
      "Processing chunk 22/181...\n",
      "Processing chunk 23/181...\n",
      "Processing chunk 24/181...\n",
      "Processing chunk 25/181...\n",
      "Processing chunk 26/181...\n",
      "Processing chunk 27/181...\n",
      "Processing chunk 28/181...\n",
      "Processing chunk 29/181...\n",
      "Processing chunk 30/181...\n",
      "Processing chunk 31/181...\n",
      "Processing chunk 32/181...\n",
      "Processing chunk 33/181...\n",
      "Processing chunk 34/181...\n",
      "Processing chunk 35/181...\n",
      "Processing chunk 36/181...\n",
      "Processing chunk 37/181...\n",
      "Processing chunk 38/181...\n",
      "Processing chunk 39/181...\n",
      "Processing chunk 40/181...\n",
      "Processing chunk 41/181...\n",
      "Processing chunk 42/181...\n",
      "Processing chunk 43/181...\n",
      "Processing chunk 44/181...\n",
      "Processing chunk 45/181...\n",
      "Processing chunk 46/181...\n",
      "Processing chunk 47/181...\n",
      "Processing chunk 48/181...\n",
      "Processing chunk 49/181...\n",
      "Processing chunk 50/181...\n",
      "Processing chunk 51/181...\n",
      "Processing chunk 52/181...\n",
      "Processing chunk 53/181...\n",
      "Processing chunk 54/181...\n",
      "Processing chunk 55/181...\n",
      "Processing chunk 56/181...\n",
      "Processing chunk 57/181...\n",
      "Processing chunk 58/181...\n",
      "Processing chunk 59/181...\n",
      "Processing chunk 60/181...\n",
      "Processing chunk 61/181...\n",
      "Processing chunk 62/181...\n",
      "Processing chunk 63/181...\n",
      "Processing chunk 64/181...\n",
      "Processing chunk 65/181...\n",
      "Processing chunk 66/181...\n",
      "Processing chunk 67/181...\n",
      "Processing chunk 68/181...\n",
      "Processing chunk 69/181...\n",
      "Processing chunk 70/181...\n",
      "Processing chunk 71/181...\n",
      "Processing chunk 72/181...\n",
      "Processing chunk 73/181...\n",
      "Processing chunk 74/181...\n",
      "Processing chunk 75/181...\n",
      "Processing chunk 76/181...\n",
      "Processing chunk 77/181...\n",
      "Processing chunk 78/181...\n",
      "Processing chunk 79/181...\n",
      "Processing chunk 80/181...\n",
      "Processing chunk 81/181...\n",
      "Processing chunk 82/181...\n",
      "Processing chunk 83/181...\n",
      "Processing chunk 84/181...\n",
      "Processing chunk 85/181...\n",
      "Processing chunk 86/181...\n",
      "Processing chunk 87/181...\n",
      "Processing chunk 88/181...\n",
      "Processing chunk 89/181...\n",
      "Processing chunk 90/181...\n",
      "Processing chunk 91/181...\n",
      "Processing chunk 92/181...\n",
      "Processing chunk 93/181...\n",
      "Processing chunk 94/181...\n",
      "Processing chunk 95/181...\n",
      "Processing chunk 96/181...\n",
      "Processing chunk 97/181...\n",
      "Processing chunk 98/181...\n",
      "Processing chunk 99/181...\n",
      "Processing chunk 100/181...\n",
      "Processing chunk 101/181...\n",
      "Processing chunk 102/181...\n",
      "Processing chunk 103/181...\n",
      "Processing chunk 104/181...\n",
      "Processing chunk 105/181...\n",
      "Processing chunk 106/181...\n",
      "Processing chunk 107/181...\n",
      "Processing chunk 108/181...\n",
      "Processing chunk 109/181...\n",
      "Processing chunk 110/181...\n",
      "Processing chunk 111/181...\n",
      "Processing chunk 112/181...\n",
      "Processing chunk 113/181...\n",
      "Processing chunk 114/181...\n",
      "Processing chunk 115/181...\n",
      "Processing chunk 116/181...\n",
      "Processing chunk 117/181...\n",
      "Processing chunk 118/181...\n",
      "Processing chunk 119/181...\n",
      "Processing chunk 120/181...\n",
      "Processing chunk 121/181...\n",
      "Processing chunk 122/181...\n",
      "Processing chunk 123/181...\n",
      "Processing chunk 124/181...\n",
      "Processing chunk 125/181...\n",
      "Processing chunk 126/181...\n",
      "Processing chunk 127/181...\n",
      "Processing chunk 128/181...\n",
      "Processing chunk 129/181...\n",
      "Processing chunk 130/181...\n",
      "Processing chunk 131/181...\n",
      "Processing chunk 132/181...\n",
      "Processing chunk 133/181...\n",
      "Processing chunk 134/181...\n",
      "Processing chunk 135/181...\n",
      "Processing chunk 136/181...\n",
      "Processing chunk 137/181...\n",
      "Processing chunk 138/181...\n",
      "Processing chunk 139/181...\n",
      "Processing chunk 140/181...\n",
      "Processing chunk 141/181...\n",
      "Processing chunk 142/181...\n",
      "Processing chunk 143/181...\n",
      "Processing chunk 144/181...\n",
      "Processing chunk 145/181...\n",
      "Processing chunk 146/181...\n",
      "Processing chunk 147/181...\n",
      "Processing chunk 148/181...\n",
      "Processing chunk 149/181...\n",
      "Processing chunk 150/181...\n",
      "Processing chunk 151/181...\n",
      "Processing chunk 152/181...\n",
      "Processing chunk 153/181...\n",
      "Processing chunk 154/181...\n",
      "Processing chunk 155/181...\n",
      "Processing chunk 156/181...\n",
      "Processing chunk 157/181...\n",
      "Processing chunk 158/181...\n",
      "Processing chunk 159/181...\n",
      "Processing chunk 160/181...\n",
      "Processing chunk 161/181...\n",
      "Processing chunk 162/181...\n",
      "Processing chunk 163/181...\n",
      "Processing chunk 164/181...\n",
      "Processing chunk 165/181...\n",
      "Processing chunk 166/181...\n",
      "Processing chunk 167/181...\n",
      "Processing chunk 168/181...\n",
      "Processing chunk 169/181...\n",
      "Processing chunk 170/181...\n",
      "Processing chunk 171/181...\n",
      "Processing chunk 172/181...\n",
      "Processing chunk 173/181...\n",
      "Processing chunk 174/181...\n",
      "Processing chunk 175/181...\n",
      "Processing chunk 176/181...\n",
      "Processing chunk 177/181...\n",
      "Processing chunk 178/181...\n",
      "Processing chunk 179/181...\n",
      "Processing chunk 180/181...\n",
      "Processing chunk 181/181...\n",
      "Parquet saved successfully\n"
     ]
    }
   ],
   "source": [
    "df_labels_output = generate_label_table(df_candidates, df_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_output = pd.read_parquet(\"data/labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_output[10000:11000].to_csv(\"test_labels_output.csv\", index=False)\n",
    "df_candidates[10000:11000].to_csv(\"test_candidates.csv\", index=False)\n",
    "df_labels[100:1100].to_csv(\"test_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          clicks  carts  orders\n",
      "0              0      0       1\n",
      "1              0      0       0\n",
      "2              0      0       0\n",
      "3              0      0       0\n",
      "4              0      0       0\n",
      "...          ...    ...     ...\n",
      "90062545       0      0       0\n",
      "90062546       0      0       0\n",
      "90062547       0      0       0\n",
      "90062548       0      0       0\n",
      "90062549       0      0       0\n",
      "\n",
      "[90062550 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_labels_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
